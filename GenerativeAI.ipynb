{
 "cells": [
  {
   "cell_type": "markdown",
  
   },
   "source": [
    "# Phase 4: Integrating Generative AI into the Restaurant Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
  
   },
   "source": [
    "# **Phase Overview**\n"
   ]
  },
  {
   "cell_type": "markdown",
  
   },
   "source": [
    "In this phase, we integrated a Generative AI model ‚ÄîLAMA‚Äî\n",
    "into the system to enhance user interaction through intelligent, human like explanations and recommendations.\n",
    "\n",
    "Instead of only presenting filtered restaurant results, the system now uses these models to generate natural language responses based on the user‚Äôs input and selected restaurant data.\n",
    "\n",
    "This allows the system to provide context-aware suggestions, explain why a certain restaurant matches the user's preferences, and offer more engaging, informative, and personalized feedback.\n",
    "\n",
    "The integration involved designing prompt templates, processing structured data into AI-readable inputs, and generating outputs that improve the overall user experience."
   ]
  },
  {
   "cell_type": "markdown",
  
   },
   "source": [
    "# **1. Dataset Preparation and Exploration**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
       "id": "br5ls9j9Izu_",
    "outputId": "f0dba1e5-e43e-4363-e4cd-976742b58d20"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"https://raw.githubusercontent.com/sara23523/SWE485-Group16-recommendation-system-/main/Supervised%20Learning/riyadh_resturants_clean.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the data\n",
    "print(\"üîπ First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check basic info\n",
    "print(\"\\nüîπ Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nüîπ Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "print(\"\\nüîπ Descriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for unique values in each column (for categoricals)\n",
    "print(\"\\nüîπ Unique values per column:\")\n",
    "for col in df.columns:\n",
    "    unique_vals = df[col].nunique()\n",
    "    print(f\"{col}: {unique_vals} unique values\")\n",
    "\n",
    "# Visualize distributions of numerical features\n",
    "numeric_cols = df.select_dtypes(include='number').columns\n",
    "df[numeric_cols].hist(bins=15, figsize=(15, 10))\n",
    "plt.suptitle(\"Distributions of Numerical Features\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap (for numerical features)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Convert categorical variables to lowercase strings for consistency\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda x: x.str.strip().str.lower())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1. Drop rows where rating is missing (if you only want rated entries)\n",
    "df_clean = df.dropna(subset=['rating'])\n",
    "\n",
    "# OR (alternative): Keep them but treat missing ratings differently\n",
    "def clean_data(df):\n",
    "    df['rating'].fillna(-1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# 2. Fill missing values in other fields\n",
    "df_clean = df.dropna(subset=['rating']).copy()\n",
    "\n",
    "df_clean.loc[:, 'ratingSignals'] = df_clean['ratingSignals'].fillna(0)\n",
    "df_clean.loc[:, 'likes'] = df_clean['likes'].fillna(0)\n",
    "\n",
    "df_clean['price'] = df_clean['price'].fillna(\"Unknown\")\n",
    "\n",
    "# 3. Optional: Log transform skewed numerical features\n",
    "for col in ['likes', 'photos', 'tips', 'ratingSignals']:\n",
    "    df_clean[f'{col}_log'] = np.log1p(df_clean[col])  # log(1 + x) to avoid log(0)\n",
    "\n",
    "# 4. Normalize/encode categorical values (if needed later)\n",
    "df_clean['price_encoded'] = df_clean['price'].map({\n",
    "    'cheap': 1,\n",
    "    'moderate': 2,\n",
    "    'expensive': 3,\n",
    "    'very expensive': 4,\n",
    "    'unknown': 0\n",
    "})\n",
    "\n",
    "# Optional: Extract top categories\n",
    "top_cats = df_clean['categories'].value_counts().nlargest(20).index\n",
    "df_clean['category_clean'] = df_clean['categories'].apply(\n",
    "    lambda x: x if x in top_cats else 'Other'\n",
    ")\n",
    "\n",
    "# 5. Preview cleaned dataset\n",
    "print()\n",
    "print()\n",
    "print(df_clean.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   
   },
   "source": [
    "# **Rationale Behind Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
      },
   "source": [
    "üîπ **Data Visualization**\n",
    "\n",
    "The heatmap explores correlations between numerical features, essential for detecting multicollinearity. Highly correlated features may need to be combined or excluded to prevent redundancy and improve model performance.\n",
    "\n",
    "üîπ **Text Normalization**\n",
    "\n",
    "String values are converted to lowercase and extra spaces removed, ensuring consistent data for better matching and grouping in further analysis.\n",
    "\n",
    "üîπ **Handling Missing Data**\n",
    "\n",
    "Dropped rows with missing rating and filled missing ratingSignals, likes, and price to ensure data completeness and integrity for analysis.\n",
    "\n",
    "üîπ **Feature Transformation (Skew Handling)**\n",
    "\n",
    "Applied log1p to right-skewed features to reduce skewness, stabilize variance, and improve model performance.\n",
    "\n",
    "üîπ **Encoding and Category Simplification**\n",
    "\n",
    "Encoded price numerically and grouped rare categories as \"Other\" to prepare data for ML models and reduce sparsity for better generalization."
   ]
  },
  {
   "cell_type": "markdown",
  
   },
   "source": [
    "# **2. Integration of Generative AI & Application of Two Different AI Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
       "id": "wkxCHWfFQGE0",
    "outputId": "1f68582a-7c20-45a4-f071-f97ebdfe55b3"
   },
   "outputs": [],
   "source": [
    "#Using FLAN-T5\n",
    "\n",
    "# Install Required Libraries\n",
    "!pip install -q transformers accelerate huggingface_hub\n",
    "\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load Dataset\n",
    "dataset_path = \"https://raw.githubusercontent.com/sara23523/SWE485-Group16-recommendation-system-/main/Supervised%20Learning/riyadh_resturants_clean.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess Dataset\n",
    "df = df.dropna(subset=['rating'])  # drop rows with missing ratings\n",
    "\n",
    "# Fill missing 'price' with most frequent value\n",
    "df['price'] = df['price'].fillna(df['price'].mode()[0])\n",
    "\n",
    "# Optional: Categorize price into descriptive labels\n",
    "price_mapping = {'$': 'Cheap', '$$': 'Moderate', '$$$': 'Expensive', '$$$$': 'Very Expensive'}\n",
    "df['price'] = df['price'].map(price_mapping).fillna('Unknown')\n",
    "\n",
    "# Display sample of cleaned data\n",
    "df_cleaned = df[['name', 'categories', 'price', 'rating']].head(10)\n",
    "df_cleaned\n",
    "\n",
    "# Load FLAN-T5 Model Pipeline\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "0\n",
    "\n",
    "# Define Prompt Templates\n",
    "\n",
    "prompt_1 = f\"\"\"\n",
    "You are a restaurant assistant. Recommend 3 restaurants from this list with a high rating and moderate price.\n",
    "\n",
    "List:\n",
    "1. {df_cleaned.iloc[0]['name']} - {df_cleaned.iloc[0]['categories']}, Price: {df_cleaned.iloc[0]['price']}, Rating: {df_cleaned.iloc[0]['rating']}\n",
    "2. {df_cleaned.iloc[1]['name']} - {df_cleaned.iloc[1]['categories']}, Price: {df_cleaned.iloc[1]['price']}, Rating: {df_cleaned.iloc[1]['rating']}\n",
    "3. {df_cleaned.iloc[2]['name']} - {df_cleaned.iloc[2]['categories']}, Price: {df_cleaned.iloc[2]['price']}, Rating: {df_cleaned.iloc[2]['rating']}\n",
    "4. {df_cleaned.iloc[3]['name']} - {df_cleaned.iloc[3]['categories']}, Price: {df_cleaned.iloc[3]['price']}, Rating: {df_cleaned.iloc[3]['rating']}\n",
    "5. {df_cleaned.iloc[4]['name']} - {df_cleaned.iloc[4]['categories']}, Price: {df_cleaned.iloc[4]['price']}, Rating: {df_cleaned.iloc[4]['rating']}\n",
    "\n",
    "Please suggest the top 3 and explain briefly why.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt 2: Ask for 3 cheap/popular places\n",
    "prompt_2 = f\"\"\"\n",
    "You are helping a tourist looking for 3 cheap and popular food places in Riyadh.\n",
    "\n",
    "Choices:\n",
    "1. {df_cleaned.iloc[5]['name']} - {df_cleaned.iloc[5]['categories']}, Price: {df_cleaned.iloc[5]['price']}, Rating: {df_cleaned.iloc[5]['rating']}\n",
    "2. {df_cleaned.iloc[6]['name']} - {df_cleaned.iloc[6]['categories']}, Price: {df_cleaned.iloc[6]['price']}, Rating: {df_cleaned.iloc[6]['rating']}\n",
    "3. {df_cleaned.iloc[7]['name']} - {df_cleaned.iloc[7]['categories']}, Price: {df_cleaned.iloc[7]['price']}, Rating: {df_cleaned.iloc[7]['rating']}\n",
    "4. {df_cleaned.iloc[8]['name']} - {df_cleaned.iloc[8]['categories']}, Price: {df_cleaned.iloc[8]['price']}, Rating: {df_cleaned.iloc[8]['rating']}\n",
    "5. {df_cleaned.iloc[9]['name']} - {df_cleaned.iloc[9]['categories']}, Price: {df_cleaned.iloc[9]['price']}, Rating: {df_cleaned.iloc[9]['rating']}\n",
    "\n",
    "Suggest 3 restaurants and explain why each is suitable for a tourist.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt 3: Ask for 3 restaurants for a family day out in Riyadh.\n",
    "prompt_3 = f\"\"\"\n",
    "You are a helpful assistant recommending restaurants for a family day out in Riyadh.\n",
    "The user is looking for family-friendly places that are budget-friendly and have decent ratings.\n",
    "Here are some options:\n",
    "\n",
    "{df_cleaned.iloc[7]['name']} - {df_cleaned.iloc[7]['categories']}, Rating: {df_cleaned.iloc[7]['rating']}, Price: {df_cleaned.iloc[7]['price']}\n",
    "{df_cleaned.iloc[8]['name']} - {df_cleaned.iloc[8]['categories']}, Rating: {df_cleaned.iloc[8]['rating']}, Price: {df_cleaned.iloc[8]['price']}\n",
    "{df_cleaned.iloc[9]['name']} - {df_cleaned.iloc[9]['categories']}, Rating: {df_cleaned.iloc[9]['rating']}, Price: {df_cleaned.iloc[9]['price']}\n",
    "{df_cleaned.iloc[0]['name']} - {df_cleaned.iloc[0]['categories']}, Rating: {df_cleaned.iloc[0]['rating']}, Price: {df_cleaned.iloc[0]['price']}\n",
    "{df_cleaned.iloc[1]['name']} - {df_cleaned.iloc[1]['categories']}, Rating: {df_cleaned.iloc[1]['rating']}, Price: {df_cleaned.iloc[1]['price']}\n",
    "\n",
    "Suggest 3 restaurants and explain briefly why they are great for families.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt 1 Response\n",
    "print(\"üîπ Prompt 1 Response (Top 3 High Rating + Moderate Price):\")\n",
    "response_1 = flan(prompt_1, max_new_tokens=200)[0]['generated_text']\n",
    "print(response_1)\n",
    "\n",
    "# Prompt 2 Response\n",
    "print(\"\\nüîπ Prompt 2 Response (Top 3 Cheap + Popular for Tourist):\")\n",
    "response_2 = flan(prompt_2, max_new_tokens=200)[0]['generated_text']\n",
    "print(response_2)\n",
    "\n",
    "# Prompt 3 Response\n",
    "print(\"üîπ Prompt 3 Responses (Family + Budget):\\n\")\n",
    "response_3 = flan(prompt_3, max_new_tokens=200)[0]['generated_text']\n",
    "print(response_3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
      },
   "outputs": [],
   "source": [
    "# Download and install Ollama server\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Pull the model\n",
    "!ollama pull llama3:8b\n",
    "\n",
    "# Install the LightRAG library with Ollama support\n",
    "!pip install -U lightrag[ollama]\n",
    "\n",
    "\n",
    "import os, threading, subprocess\n",
    "\n",
    "# Set environment variables for Ollama to allow external access and cross-origin requests\n",
    "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "\n",
    "# Function to start the Ollama API server in the background\n",
    "def ollama():\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "# Start the Ollama server in a separate thread so it doesn't block execution\n",
    "threading.Thread(target=ollama).start()\n",
    "\n",
    "# Import core LightRAG components\n",
    "from lightrag.core.generator import Generator\n",
    "from lightrag.core.component import Component\n",
    "from lightrag.core.model_client import ModelClient\n",
    "from lightrag.components.model_client import OllamaClient\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Template for the prompt sent to the model\n",
    "qa_template = r\"\"\"<SYS>\n",
    "You are a helpful assistant.\n",
    "</SYS>\n",
    "User: {{input_str}}\n",
    "You:\"\"\"\n",
    "\n",
    "# Define a simple QA component using the template and model client\n",
    "class SimpleQA(Component):\n",
    "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            template=qa_template,\n",
    "        )\n",
    "\n",
    "    # Synchronous call method\n",
    "    def call(self, input: dict) -> str:\n",
    "        return self.generator.call({\"input_str\": str(input)})\n",
    "\n",
    "# Create the QA system using the Ollama model\n",
    "model = {\n",
    "    \"model_client\": OllamaClient(),\n",
    "    \"model_kwargs\": {\"model\": \"llama3:8b\"}\n",
    "}\n",
    "\n",
    "# Initialize the SimpleQA component with the model\n",
    "qa = SimpleQA(**model)\n",
    "\n",
    "# Send a sample question to the model\n",
    "output = qa(\"what is happiness\")\n",
    "\n",
    "# Display the model's response as Markdown\n",
    "display(Markdown(f\"**Answer:** {output.data}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",

   "source": [
    "# **3. Simulating User Interactions for Restaurant Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
       "id": "OC6xuYaNaO48",
    "outputId": "43a9be65-8df1-4111-c9e9-617fea2f8cbc"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate huggingface_hub\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load dataset\n",
    "dataset_url = \"https://raw.githubusercontent.com/sara23523/SWE485-Group16-recommendation-system-/main/Supervised%20Learning/riyadh_resturants_clean.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# Preprocess\n",
    "df = df.dropna(subset=['rating'])\n",
    "df['price'] = df['price'].fillna(df['price'].mode()[0])\n",
    "price_map = {'$': 'Cheap', '$$': 'Moderate', '$$$': 'Expensive', '$$$$': 'Very Expensive'}\n",
    "df['price'] = df['price'].map(price_map).fillna('Unknown')\n",
    "\n",
    "# Load FLAN-T5\n",
    "flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
      "source": [
    "###Define Prompt Builder from User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
      "outputs": [],
   "source": [
    "def build_prompt_from_query(user_query, filtered_df):\n",
    "    restaurant_list = []\n",
    "    for idx, row in filtered_df.iterrows():\n",
    "        restaurant_list.append(f\"{row['name']} - {row['categories']}, Price: {row['price']}, Rating: {row['rating']}\")\n",
    "    restaurant_str = \"\\n\".join(restaurant_list[:5])  # Use top 5 for brevity\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful AI assistant recommending restaurants in Riyadh.\n",
    "\n",
    "User query: \"{user_query}\"\n",
    "\n",
    "Here are some restaurant options:\n",
    "{restaurant_str}\n",
    "\n",
    "Based on the user‚Äôs input, suggest 3 restaurants and explain briefly why each is a good choice.\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
      "source": [
    " ### Simulate User Interaction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
      "outputs": [],
   "source": [
    "def simulate_user_interaction():\n",
    "    user_query = input(\"üßë‚Äçüí¨ What are you looking for? (e.g., 'cheap places with good reviews for families'): \").strip().lower()\n",
    "\n",
    "    # Basic keyword filtering\n",
    "    if \"cheap\" in user_query:\n",
    "        filtered_df = df[df['price'] == 'Cheap']\n",
    "    elif \"moderate\" in user_query:\n",
    "        filtered_df = df[df['price'] == 'Moderate']\n",
    "    elif \"expensive\" in user_query:\n",
    "        filtered_df = df[df['price'].isin(['Expensive', 'Very Expensive'])]\n",
    "    else:\n",
    "        filtered_df = df.copy()\n",
    "\n",
    "    if \"high rating\" in user_query or \"good reviews\" in user_query:\n",
    "        filtered_df = filtered_df[filtered_df['rating'] >= 4.0]\n",
    "    elif \"decent\" in user_query or \"okay\" in user_query:\n",
    "        filtered_df = filtered_df[filtered_df['rating'] >= 3.5]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"‚ùå No restaurants match the current search criteria.\")\n",
    "        return\n",
    "\n",
    "    # Sort and get top N for prompt\n",
    "    filtered_df = filtered_df.sort_values(by=\"rating\", ascending=False).head(5)\n",
    "    prompt = build_prompt_from_query(user_query, filtered_df)\n",
    "\n",
    "    print(\"\\nüì® Generated Prompt:\\n\", prompt)\n",
    "\n",
    "    print(\"\\nü§ñ FLAN-T5 Response:\\n\")\n",
    "    response = flan(prompt, max_new_tokens=200)[0]['generated_text']\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
       "id": "iRaOIH8DzFZu",
    "outputId": "259624be-fa0d-47e0-9d3d-0453627dd24c"
   },
   "outputs": [],
   "source": [
    "simulate_user_interaction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
       "id": "1EXcS7LQbzqD",
    "outputId": "2a8d9ebb-ba03-4cdf-ec40-b1025ab285ad"
   },
   "outputs": [],
   "source": [
    "simulate_user_interaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
       "id": "4F0nRW9OcJCx",
    "outputId": "26a20d33-c8ea-428a-a4c1-04c8a29bd264"
   },
   "outputs": [],
   "source": [
    "simulate_user_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
      "source": [
    "# **4. Template Differences and Selection Justification**"
   ]
  },
  {
   "cell_type": "markdown",
      "source": [
    "In this phase, three distinct prompt templates were designed and applied to the FLAN-T5 model to recommend restaurants in Riyadh based on different user preferences and contextual requirements. Each template followed a specific scenario and instruction set to test the model‚Äôs flexibility in generating appropriate recommendations.\n",
    "\n",
    "### Prompt Template Overview:\n",
    "\n",
    "| Template | Scenario | User Preference | Structure | Selection Criteria |\n",
    "|:------------|:------------|:-----------------|:----------------|:------------------|\n",
    "| **Prompt 1** | Recommend top 3 restaurants | High rating and moderate price | Direct list of options with ratings and prices, asking for recommendations and brief reasoning | Assess model‚Äôs ability to prioritize based on rating and price |\n",
    "| **Prompt 2** | Suggest 3 cheap and popular places for tourists | Cheap price and popularity | Direct list with explicit cheap price filtering | Test model‚Äôs sensitivity to budget-friendly filtering |\n",
    "| **Prompt 3** | Recommend family-friendly restaurants | Budget-friendly and decent rating for families | Casual instruction with list of options and emphasis on family suitability | Evaluate model‚Äôs ability to address context-specific needs |\n",
    "\n",
    "---\n",
    "\n",
    "### Differences in Template Logic:\n",
    "- **Prompt 1** focuses on sorting based on numeric rating and a specific price tier.\n",
    "- **Prompt 2** shifts the focus towards affordability and implied popularity.\n",
    "- **Prompt 3** introduces contextual suitability (family-friendly) in addition to price and rating.\n",
    "\n",
    "---\n",
    "\n",
    "### Selection Justification:\n",
    "Using multiple prompt templates allowed for a comprehensive evaluation of the model's capacity to adapt to various user needs and contextual cues. The diversity in prompt structures highlights how phrasing and emphasis affect the AI-generated responses.  \n",
    "\n",
    "Among these, **Prompt 1** and **Prompt 3** demonstrated superior balance between precision and relevance:\n",
    "- **Prompt 1** reliably filtered based on quantitative values (ratings and prices).\n",
    "- **Prompt 3** effectively handled context-sensitive advice (family outings), making it valuable for real-world assistant scenarios.\n",
    "\n",
    "As a result, these templates were identified as most aligned with the project‚Äôs objective of delivering tailored, explainable recommendations through Generative AI integration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
      "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
      "source": [
    "The integration of FLAN-T5 with custom prompt templates successfully demonstrated the potential of Generative AI in providing dynamic, personalized restaurant recommendations based on varied user preferences. The experiment confirmed that template design significantly influences the quality and relevance of AI-generated content.\n",
    "\n",
    "By experimenting with multiple prompt structures, it was evident that:\n",
    "- **Template 1** excelled in structured, value-based recommendations.\n",
    "- **Template 3** provided effective context-driven advice suitable for specialized user needs.\n",
    "\n",
    "This phase emphasized the importance of prompt engineering in Generative AI applications, particularly for recommendation systems where user preferences and context critically affect output quality.  \n",
    "\n",
    "---"
   ]
  }
 ],
   "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
